import os
import pandas as pd

from scripts.utils import settings
from snakemake.utils import validate

env_configs = settings.env_configs

IMPORTDIR = env_configs['import_dir']
PROCESSINGDIR = env_configs['processing_dir'].replace('/','.')
SNAKEMAKELOGS = env_configs['snakemake_logs']
NEO4J_LOGDIR = env_configs['log_dir']
CONTAINER_NAME = env_configs["container_name"]
CONFIG_PATH = env_configs["config_path"]

NODEDIR = 'nodes'
RELDIR = 'rels'

THREADS=env_configs['threads']

configfile: os.path.join(CONFIG_PATH,"data_integration.yaml")

#validata configfile
#maybe add this as a rule
if NODEDIR in config:
    nodes = config[NODEDIR]
    for i in nodes:
        validate(nodes[i], "../config/data_integration.schema.yaml")
if RELDIR in config:
    rels = config[RELDIR]
    for i in rels:
        validate(rels[i], "../config/data_integration.schema.yaml")    

rule all:
    input: f"{SNAKEMAKELOGS}/master_import.log"

#remove all intermediate files
rule clean_all:
    log: f"{SNAKEMAKELOGS}/clean_all.log"
    params:
        IMPORTDIR = IMPORTDIR,
        NODEDIR = NODEDIR,
        RELDIR = RELDIR,
        SNAKEMAKELOGS = SNAKEMAKELOGS
    shell:
        """
        echo 'Deleting {params.IMPORTDIR}/{params.NODEDIR}/merged/*'
        rm -f {params.IMPORTDIR}/{params.NODEDIR}/merged/*
        echo 'Deleting {params.IMPORTDIR}/{params.NODEDIR}/created.txt'
        rm -f {params.IMPORTDIR}/{params.NODEDIR}/created.txt
        echo 'Deleting {params.IMPORTDIR}/{params.RELDIR}/created.txt'
        rm -f {params.IMPORTDIR}/{params.RELDIR}/created.txt
        echo 'Deleting find {params.SNAKEMAKELOGS}/{params.NODEDIR} -name "*.log" -delete'
        find {params.SNAKEMAKELOGS}/{params.NODEDIR} -name "*.log" -delete
        echo 'Deleting find {params.SNAKEMAKELOGS}/{params.RELDIR} -name "*.log" -delete'
        find {params.SNAKEMAKELOGS}/{params.RELDIR} -name "*.log" -delete
        echo 'Deleting {params.IMPORTDIR}/master*'
        rm -f {params.IMPORTDIR}/master*
        echo 'Deleting {params.SNAKEMAKELOGS}/master*'
        rm -f {params.SNAKEMAKELOGS}/master*
        echo 'Deleting {params.IMPORTDIR}/logs/*'
        rm -f {params.IMPORTDIR}/logs/*
        #not sure if below is too severe
        echo 'Deleting find {params.IMPORTDIR}/{params.NODEDIR} -name "*.csv.gz" -delete'
        find {params.IMPORTDIR}/{params.NODEDIR} -name "*.csv.gz" -delete
        echo 'Deleting find {params.IMPORTDIR}/{params.RELDIR} -name "*.csv.gz" -delete'
        find {params.IMPORTDIR}/{params.RELDIR} -name "*.csv.gz" -delete
        """

#remove all intermediate files
rule clean_for_build:
    log: f"{SNAKEMAKELOGS}/clean_for_build.log"
    params:
        IMPORTDIR = IMPORTDIR,
        NODEDIR = NODEDIR,
        RELDIR = RELDIR
    shell:
        """
        rm -f {params.IMPORTDIR}/{params.NODEDIR}/merged/*
        rm -f {params.IMPORTDIR}/{params.NODEDIR}/created.txt
        rm -f {params.IMPORTDIR}/{params.RELDIR}/created.txt
        rm -f {params.IMPORTDIR}/master*
        """       

rule check_new_data:
    log: f"{SNAKEMAKELOGS}/check_new_data.log"
    input: 
        expand(os.path.join(IMPORTDIR,NODEDIR,'{node}','{node}.csv.gz'), node = config[NODEDIR]),
        expand(os.path.join(IMPORTDIR,RELDIR,'{rel}','{rel}.csv.gz'), rel = config[RELDIR])

rule create_graph:
    log: f"{SNAKEMAKELOGS}/create_graph.log"
    input: f"{IMPORTDIR}/master_import.sh"
    output: f"{SNAKEMAKELOGS}/master_import.log"
    shell:
        """
        docker-compose up -d --no-recreate
        echo 'removing old database...'
        docker exec --user neo4j {CONTAINER_NAME} sh -c 'rm -rf /var/lib/neo4j/data/databases/neo4j'
        docker exec --user neo4j {CONTAINER_NAME} sh -c 'rm -f /var/lib/neo4j/data/transactions/neo4j/*'
        echo 'running import...'
        SECONDS=0
        docker exec --user neo4j {CONTAINER_NAME} sh /var/lib/neo4j/import/master_import.sh > {SNAKEMAKELOGS}/master_import.log
        duration=$SECONDS
        echo "Import took $(($duration / 60)) minutes and $(($duration % 60)) seconds."
        echo 'stopping container...'
        docker stop {CONTAINER_NAME}
        echo 'starting container...'
        docker start {CONTAINER_NAME}
        echo 'waiting a bit...'
        sleep 30
        echo 'adding contraints and extra bits...'
        docker exec --user neo4j {CONTAINER_NAME} sh /var/lib/neo4j/import/master_constraints.sh > {SNAKEMAKELOGS}/master_constraints.log
        echo 'waiting a bit for indexes to populate...'
        sleep 30
        echo 'checking import report...'
        python -m workflow.scripts.graph_build.import-report-check {NEO4J_LOGDIR}/import.report > {SNAKEMAKELOGS}/master_import.log
        """

rule prepare_for_load:
    input: 
        os.path.join(IMPORTDIR,NODEDIR,'created.txt'),
        os.path.join(IMPORTDIR,RELDIR,'created.txt')
    output: f"{IMPORTDIR}/master_import.sh"
    shell: 
        """
        rm -f {IMPORTDIR}/{NODEDIR}/merged/*
        python -m workflow.scripts.graph_build.merge_sources 
        python -m workflow.scripts.graph_build.create_master_import
        """

rule read_config_nodes:
    """
    Get the node information from the config file
    """
    #threads: workflow.cores * 0.75
    input: expand(os.path.join(IMPORTDIR,NODEDIR,'{node}','{node}.csv.gz'), node = config[NODEDIR])
    output: os.path.join(IMPORTDIR,NODEDIR,'created.txt')
    shell: "echo `date` > {IMPORTDIR}/{NODEDIR}/created.txt"

rule process_nodes:
    """
    Process each node
    """
    output: os.path.join(IMPORTDIR,NODEDIR,'{node}','{node}.csv.gz')
    params:
        metaData = lambda wildcards: config[NODEDIR][wildcards.node],
        meta_id = lambda wildcards: wildcards.node,
        PROCESSINGDIR=PROCESSINGDIR
    shell: 
        """
        #make neo4j directory
        d={IMPORTDIR}/{NODEDIR}/{params.meta_id}
        mkdir -p $d
        
        #clean up any old import and constraint data
        rm -f $d/{params.meta_id}-import-nodes.txt
        rm -f $d/{params.meta_id}-constraint.txt

        #run the processing script
        python -m {params.PROCESSINGDIR}.{params.metaData[script]} -n {params.meta_id} 
        """

rule read_config_rels:
    """
    Get the rel information from the config file
    """
    #threads: workflow.cores * 0.75
    input: expand(os.path.join(IMPORTDIR,RELDIR,'{rel}','{rel}.csv.gz'), rel = config[RELDIR])
    output: os.path.join(IMPORTDIR,RELDIR,'created.txt')
    shell: "echo `date` > {IMPORTDIR}/{RELDIR}/created.txt"

rule process_rels:
    """
    Process each rel
    """
    output: os.path.join(IMPORTDIR,RELDIR,'{rel}','{rel}.csv.gz')
    params:
        metaData = lambda wildcards: config[RELDIR][wildcards.rel],
        meta_id = lambda wildcards: wildcards.rel,
        PROCESSINGDIR=PROCESSINGDIR
    shell: 
        """
        #make directory
        d={IMPORTDIR}/{RELDIR}/{params.meta_id}
        mkdir -p $d
        
        #clean up any old import and constraint data
        rm -f $d/{params.meta_id}-import-rels.txt
        rm -f $d/{params.meta_id}-constraint.txt
        
        #run the processing script
        python -m {params.PROCESSINGDIR}.{params.metaData[script]} -n {params.meta_id} 
        """
